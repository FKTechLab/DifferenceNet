# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_model.ipynb (unless otherwise specified).

__all__ = ['Difference', 'TrajectoryDN', 'TrajectorySN', 'ContrastiveLoss', 'contastive_loss']

# Cell
import torch
import torch.nn as nn
import numpy as np

# Cell
class Difference(nn.Module):
    '''Difference Layer'''
    def __init__(self, mode = 'simple'):
        super(Difference, self).__init__()
        self.mode = mode

    def forward(self, x1, x2):
        '''Difference Layer: supports 3 types (simple, abs, square)'''
        if self.mode == 'simple':
            return x1 - x2
        elif self.mode == 'abs':
            return torch.abs(x1 - x2)
        elif self.mode == 'square':
            return torch.square(x1 - x2)
        else:
            raise NotImplementedError

# Cell
class TrajectoryDN(nn.Module):
    '''Trajectory Identification using Difference Net'''
    def __init__(self, diff_net, n_features):
        super(TrajectoryDN, self).__init__()
        self.diff_net = diff_net
        self.n_features = n_features
        self.n_hidden_rnn = 64 # number of hidden states for RNN
        self.n_layers_rnn = 2 # number of RNN layers (stacked)
        self.lstm = torch.nn.LSTM(
            input_size = self.n_features,
            hidden_size = self.n_hidden_rnn,
            num_layers = self.n_layers_rnn,
            batch_first = True
        )
        self.fcn = nn.Sequential(
            nn.Linear(self.n_hidden_rnn, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )

    def forward(self, x1, x2, x_seq_lens):
        # DifferenceNet
        x_diff_org = self.diff_net(x1, x2[0]) # (B X T X F)

        x_diff_dest = self.diff_net(x1, x2[1]) # (B X T X F)
        x = torch.cat((x_diff_org, x_diff_dest), dim=2) # (B X T X F*2)
        # Pack padded batch of sequences for RNN module
        packed = nn.utils.rnn.pack_padded_sequence(x, x_seq_lens.cpu(), batch_first=True, enforce_sorted=False)
        # Forward pass through LSTM
        outputs, (hidden, cell) = self.lstm(packed)
        # FCN
        x = hidden[-1] # Use the hidden state from the last LSTM in a stacked LSTM
        x = self.fcn(x)
        return x

# Cell
class TrajectorySN(nn.Module):
    '''Trajectory Identification using Siamese Net'''
    def __init__(self, n_features, embed_size=16):
        super(TrajectorySN, self).__init__()
        self.n_features = n_features
        self.embed_size = embed_size
        self.n_hidden_rnn = 64 # number of hidden states for RNN
        self.n_layers_rnn = 2 # number of RNN layers (stacked)
        self.lstm = torch.nn.LSTM(
            input_size = self.n_features,
            hidden_size = self.n_hidden_rnn,
            num_layers = self.n_layers_rnn,
            batch_first = True
        )
        self.traj_embed = nn.Sequential(
            nn.Linear(self.n_hidden_rnn, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, self.embed_size),
            nn.ReLU()
        )
        self.stop_embed = nn.Sequential(
            nn.Linear(self.n_features*2, 32), # 2x for org and dest
            nn.ReLU(),
            nn.Linear(32, self.embed_size),
            nn.ReLU()
        )

    def forward(self, x1, x2, x_seq_lens):
        # Siam 1: x1
        # Pack padded batch of sequences for RNN module
        packed = nn.utils.rnn.pack_padded_sequence(
            x1,
            x_seq_lens,
            batch_first=True,
            enforce_sorted=False
        )
        # Forward pass through LSTM
        outputs, (hidden, cell) = self.lstm(packed)
        # FCN
        x1 = hidden[-1] # Use the hidden state from the last LSTM in a stacked LSTM
        x1 = self.traj_embed(x1)
        # Siamese Prep: x2
        x20 = x2[0][:,-1,:]
        x21 = x2[1][:,-1,:]
        x2 = torch.cat((x20, x21), dim=1)
        # Siam 2: x2
        x2 = self.stop_embed(x2)
        return x1, x2

# Cell
class ContrastiveLoss(nn.Module):
    '''Contrastive Loss'''
    def __init__(self, margin=1.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
        # Find the pairwise distance or eucledian distance of two output feature vectors
        euclidean_distance = torch.pairwise_distance(output1, output2)
        # perform contrastive loss calculation with the distance
        pos_ = (1-label) * torch.pow(euclidean_distance, 2)
        neg_ = label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)
        loss_contrastive = torch.mean(pos_ + neg_)
        return loss_contrastive

# Cell
def contastive_loss(x1, x2, y, margin=1.0):
    distance = torch.pairwise_distance(x1, x2)
    pos_part = (1.0 - y) * torch.pow(distance, 2)
    neg_part = y * torch.relu(torch.pow(margin - distance, 2))
    loss = 0.5 * (pos_part + neg_part)
    return loss.mean()